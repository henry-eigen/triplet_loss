{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout, Convolution2D, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform,he_uniform\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from custom_lib.triplet_utils import buildDataSet, build_model\n",
    "from custom_lib.triplet_utils import get_batch_hard, compute_probs\n",
    "from custom_lib.triplet_utils import add_top, remove_top\n",
    "\n",
    "from art.classifiers import KerasClassifier\n",
    "from art.attacks.projected_gradient_descent import ProjectedGradientDescent\n",
    "from art.attacks.iterative_method import BasicIterativeMethod\n",
    "from art.defences.adversarial_trainer import AdversarialTrainer\n",
    "from art.attacks.fast_gradient import FastGradientMethod\n",
    "\n",
    "from custom_lib.build_resnet import resnet_v1, resnet_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_cpu():\n",
    "    p = psutil.Process()\n",
    "\n",
    "    for i in p.threads():\n",
    "        temp = psutil.Process(i.id)\n",
    "\n",
    "        temp.cpu_affinity([i for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "val_train = to_categorical(y_train, 10)\n",
    "val_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-6\n",
    "batch_size=64\n",
    "epochs=10\n",
    "\n",
    "nb_classes = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "in_shape = input_shape\n",
    "\n",
    "#dataset_train, dataset_test, x_train_origin, y_train_origin, x_test_origin, y_test_origin = buildDataSet(cifar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Base Model --------------------------\n",
    "\n",
    "base_model = resnet_emb(input_shape)\n",
    "base_model.load_weights(\"saved_models/old_base.h5\")\n",
    "\n",
    "base_in = Input(shape=in_shape)\n",
    "base_out = base_model(base_in)\n",
    "\n",
    "# -------------------- New Model ---------------------------\n",
    "\n",
    "new_out = Dense(10, activation='softmax', name=\"cat_out\")(base_out)\n",
    "\n",
    "model = Model(inputs=base_in, \n",
    "              outputs=new_out, \n",
    "              name=\"fullnet\")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(5e-4), metrics=[\"accuracy\"])\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_top.fit(x_train, val_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = KerasClassifier(clip_values=(0, 1.), model=with_top, use_logits=False)\n",
    "attack = BasicIterativeMethod(top_model, eps=0.04, eps_step=0.01, max_iter=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_batch_generator(X, Y, batch_size = 64):\n",
    "    indices = np.arange(len(X)) \n",
    "    batch=[]\n",
    "    while True:\n",
    "            # it might be a good idea to shuffle your data before each epoch\n",
    "            np.random.shuffle(indices) \n",
    "            for i in indices:\n",
    "                batch.append(i)\n",
    "                if len(batch)==batch_size:\n",
    "                    adv_x = X[batch]\n",
    "                    adv_x[:32] = attack.generate(adv_x[:32])\n",
    "                    yield adv_x, Y[batch]\n",
    "                    batch=[]\n",
    "\n",
    "train_generator = adv_batch_generator(x_train, val_train, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "#K.set_value(adv_model.optimizer.lr, 1e-8)\n",
    "\n",
    "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
    "n_iter = 15000 # No. of training iterations\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "n_iteration=0\n",
    "\n",
    "loss_list = []\n",
    "emb_vals = []\n",
    "cat_vals = []\n",
    "\n",
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for i in range(1, n_iter+1):\n",
    "    \n",
    "    batch = train_generator.__next__()\n",
    "    \n",
    "    loss = adv_model.train_on_batch(batch[0], batch[1])\n",
    "    \n",
    "    emb_vals.append(loss[0])\n",
    "    cat_vals.append(loss[1])\n",
    "    \n",
    "    n_iteration += 1\n",
    "    \n",
    "    if i % 5000 == 0:\n",
    "        K.set_value(model.optimizer.lr, K.get_value(model.optimizer.lr)/5.0)\n",
    "    \n",
    "    if i % evaluate_every == 0:\n",
    "        ploss = (np.mean(emb_vals), np.mean(cat_vals))\n",
    "        loss_list.append(ploss)\n",
    "        emb_vals = []\n",
    "        cat_vals = []\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"[{3}] Time for {0} iterations: {1:.1f} mins, Train Loss: {2}\".format(i, (time.time()-t_start)/60.0,ploss,n_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.zeros(shape=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    outs_1 = pen_output([dataset_train[i]])[0]\n",
    "    print(i)\n",
    "    for j in range(10):\n",
    "        outs_2 = pen_output([attack.generate(dataset_train[j][:2000])])[0]\n",
    "        norms = []\n",
    "        for k in range(1000):\n",
    "            norms.append(norm(outs_1[\n",
    "                np.random.randint(0, 5000)] - outs_2[\n",
    "                np.random.randint(0, 2000)]))\n",
    "        dist_matrix[i][j] = np.mean(norms)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "\n",
    "print(dist_matrix.round(3))\n",
    "\n",
    "plt.imshow(dist_matrix / dist_matrix.max(), cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
