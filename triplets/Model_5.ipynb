{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout, Convolution2D, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform,he_uniform\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "from custom_lib.triplet_utils import buildDataSet, build_model\n",
    "from custom_lib.triplet_utils import get_batch_hard, compute_probs\n",
    "from custom_lib.triplet_utils import add_top, remove_top\n",
    "\n",
    "from art.classifiers import KerasClassifier\n",
    "from art.attacks.projected_gradient_descent import ProjectedGradientDescent\n",
    "from art.attacks.iterative_method import BasicIterativeMethod\n",
    "from art.defences.adversarial_trainer import AdversarialTrainer\n",
    "from art.attacks.fast_gradient import FastGradientMethod\n",
    "\n",
    "from custom_lib.build_resnet import resnet_v1, resnet_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_cpu():\n",
    "    p = psutil.Process()\n",
    "\n",
    "    for i in p.threads():\n",
    "        temp = psutil.Process(i.id)\n",
    "\n",
    "        temp.cpu_affinity([i for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-6\n",
    "batch_size=64\n",
    "epochs=10\n",
    "\n",
    "nb_classes = 10\n",
    "img_rows, img_cols = 32, 32\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "in_shape = input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/heigen/.conda/envs/cenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/heigen/.conda/envs/cenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 241.50 191.00\" width=\"242pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 237.5,-187 237.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140220817667296 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140220817667296</title>\n",
       "<polygon fill=\"none\" points=\"59,-146.5 59,-182.5 184,-182.5 184,-146.5 59,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-160.8\">input_2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140220817667072 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140220817667072</title>\n",
       "<polygon fill=\"none\" points=\"67.5,-73.5 67.5,-109.5 175.5,-109.5 175.5,-73.5 67.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-87.8\">model_1: Model</text>\n",
       "</g>\n",
       "<!-- 140220817667296&#45;&gt;140220817667072 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140220817667296-&gt;140220817667072</title>\n",
       "<path d=\"M121.5,-146.313C121.5,-138.289 121.5,-128.547 121.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"125,-119.529 121.5,-109.529 118,-119.529 125,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140220817779064 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140220817779064</title>\n",
       "<polygon fill=\"none\" points=\"0,-0.5 0,-36.5 117,-36.5 117,-0.5 0,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"58.5\" y=\"-14.8\">emb_out: Lambda</text>\n",
       "</g>\n",
       "<!-- 140220817667072&#45;&gt;140220817779064 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140220817667072-&gt;140220817779064</title>\n",
       "<path d=\"M106.249,-73.3129C98.511,-64.5918 88.9709,-53.8402 80.4576,-44.2459\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"82.8652,-41.6858 73.6101,-36.5288 77.6293,-46.3318 82.8652,-41.6858\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140227688907160 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140227688907160</title>\n",
       "<polygon fill=\"none\" points=\"135.5,-0.5 135.5,-36.5 233.5,-36.5 233.5,-0.5 135.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184.5\" y=\"-14.8\">cat_out: Dense</text>\n",
       "</g>\n",
       "<!-- 140220817667072&#45;&gt;140227688907160 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140220817667072-&gt;140227688907160</title>\n",
       "<path d=\"M136.751,-73.3129C144.489,-64.5918 154.029,-53.8402 162.542,-44.2459\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"165.371,-46.3318 169.39,-36.5288 160.135,-41.6858 165.371,-46.3318\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- Base Model --------------------------\n",
    "\n",
    "base_model = resnet_emb(input_shape)\n",
    "base_model.load_weights(\"saved_models/old_base.h5\")\n",
    "\n",
    "base_in = Input(shape=in_shape)\n",
    "base_out = base_model(base_in)\n",
    "\n",
    "# -------------------- Emb Model ---------------------------\n",
    "\n",
    "emb_branch = Lambda(lambda t: K.l2_normalize(t,axis=-1), name=\"emb_out\")(base_out)\n",
    "#emb_branch = Lambda(lambda t: t, name=\"emb_out\")(base_out)\n",
    "                                                  \n",
    "# -------------------- Categorical Model -------------------\n",
    "\n",
    "cat_branch = Dense(10, activation='softmax', name=\"cat_out\")(base_out)\n",
    "\n",
    "# -------------------- Compile -----------------------------\n",
    "\n",
    "model = Model(inputs=base_in, \n",
    "              outputs=[emb_branch, cat_branch], \n",
    "              name=\"fullnet\")\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    \"emb_out\": euclidean_distance_loss,\n",
    "    \"cat_out\": \"categorical_crossentropy\",\n",
    "}\n",
    "\n",
    "lossWeights = {\"emb_out\": 5, \"cat_out\": 0.2}\n",
    "\n",
    "#model.compile(optimizer=RMSprop(lr=5e-4), loss=losses, loss_weights=lossWeights, metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=Adam(lr=5e-4), loss=losses, loss_weights=lossWeights, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_in = Input(shape=in_shape)\n",
    "b_out = model(b_in)[1]\n",
    "\n",
    "adv_model = Model(inputs=b_in, outputs=b_out)\n",
    "adv_model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(1e-3), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/heigen/.conda/envs/cenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 0.2801 - emb_out_loss: 1.0000 - cat_out_loss: 0.3280 - emb_out_acc: 0.0000e+00 - cat_out_acc: 0.8944 - val_loss: 0.2942 - val_emb_out_loss: 1.0000 - val_cat_out_loss: 0.3521 - val_emb_out_acc: 0.0000e+00 - val_cat_out_acc: 0.8820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f895c557dd8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_train = np.zeros(shape=(x_train.shape[0], 128))\n",
    "dummy_test = np.zeros(shape=(x_test.shape[0], 128))\n",
    "\n",
    "model.fit(x_train, [dummy_train, y_train],\n",
    "          validation_data=(x_test, [dummy_test, y_test]),\n",
    "          epochs=1, \n",
    "          batch_size=32,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/heigen/.conda/envs/cenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3298: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "art_model = KerasClassifier(clip_values=(0, 1.), model=adv_model, use_logits=False)\n",
    "attack = ProjectedGradientDescent(art_model, norm=2, eps=8, random_eps=True, eps_step=2, max_iter=5, batch_size=16)\n",
    "\n",
    "adv_test = attack.generate(x_test[:10])\n",
    "\n",
    "restrict_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train = model.predict(x_train)[0]\n",
    "emb_test = model.predict(x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(attack.classifier._model.layers[1].layers[1].layers[-8].get_weights()[0][0, 0, :4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size = 64):\n",
    "    \n",
    "    indices = np.arange(len(X))\n",
    "    \n",
    "    batch=[]\n",
    "    while True:\n",
    "            # it might be a good idea to shuffle your data before each epoch\n",
    "            np.random.shuffle(indices) \n",
    "            for i in indices:\n",
    "                batch.append(i)\n",
    "                if len(batch)==batch_size:\n",
    "                    adv_x = X[batch]\n",
    "                    VAL = model.predict(adv_x)[0]\n",
    "                    adv_x[:48] = attack.generate(adv_x[:48])\n",
    "                    yield adv_x, [VAL, Y[batch]]\n",
    "                    batch=[]\n",
    "\n",
    "train_generator = batch_generator(x_train, y_train, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size = 64):\n",
    "    \n",
    "    indices = np.arange(len(X))\n",
    "    \n",
    "    batch=[]\n",
    "    while True:\n",
    "            # it might be a good idea to shuffle your data before each epoch\n",
    "            np.random.shuffle(indices) \n",
    "            for i in indices:\n",
    "                batch.append(i)\n",
    "                if len(batch)==batch_size:\n",
    "                    adv_x = X[batch]\n",
    "                    VAL = model.predict(adv_x)[0]\n",
    "                    adv_x = attack.generate(adv_x)\n",
    "                    \n",
    "                    x_full = np.concatenate((X[batch], adv_x), axis=0)\n",
    "                    e_full = np.concatenate((VAL, VAL), axis=0)\n",
    "                    y_full = np.concatenate((Y[batch], Y[batch]), axis=0)\n",
    "                    \n",
    "                    yield x_full, [e_full, y_full]\n",
    "                    batch=[]\n",
    "\n",
    "train_generator = batch_generator(x_train, y_train, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.0667241252958775\n",
      "emb_out_loss 0.5259471535682678\n",
      "cat_out_loss 7.638870066031814\n",
      "emb_out_acc 0.5546875\n",
      "cat_out_acc 0.5\n"
     ]
    }
   ],
   "source": [
    "batch = train_generator.__next__()\n",
    "\n",
    "loss = model.evaluate(batch[0], batch[1], verbose=0)\n",
    "\n",
    "for i, l in enumerate(model.metrics_names):\n",
    "    print(l, loss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "[125] Time for 125 iterations: 1.3 mins, Train Loss: (0.010755617, 0.9395914, 0.677)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[250] Time for 250 iterations: 2.3 mins, Train Loss: (0.008307475, 0.91911167, 0.6826875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[375] Time for 375 iterations: 3.3 mins, Train Loss: (0.007650793, 0.9356564, 0.66975)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[500] Time for 500 iterations: 4.3 mins, Train Loss: (0.008966271, 0.90459025, 0.6858125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[625] Time for 625 iterations: 5.3 mins, Train Loss: (0.0073885294, 0.92321527, 0.678875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[750] Time for 750 iterations: 6.3 mins, Train Loss: (0.008228601, 0.909911, 0.684125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[875] Time for 875 iterations: 7.2 mins, Train Loss: (0.094695516, 1.1937257, 0.603625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1000] Time for 1000 iterations: 8.2 mins, Train Loss: (0.026592605, 1.8639022, 0.3404375)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1125] Time for 1125 iterations: 9.3 mins, Train Loss: (0.03678295, 1.363311, 0.549125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1250] Time for 1250 iterations: 10.3 mins, Train Loss: (0.025358472, 1.0214586, 0.659625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1375] Time for 1375 iterations: 11.3 mins, Train Loss: (0.025310686, 1.0570517, 0.6304375)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1500] Time for 1500 iterations: 12.3 mins, Train Loss: (0.057373542, 1.0986668, 0.6540625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1625] Time for 1625 iterations: 13.3 mins, Train Loss: (0.039652348, 0.9938175, 0.6688125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1750] Time for 1750 iterations: 14.2 mins, Train Loss: (0.10942503, 1.158471, 0.6145)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[1875] Time for 1875 iterations: 15.2 mins, Train Loss: (0.31517735, 1.4215193, 0.5553125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2000] Time for 2000 iterations: 16.2 mins, Train Loss: (0.09133651, 2.131591, 0.2080625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2125] Time for 2125 iterations: 17.2 mins, Train Loss: (0.05470023, 2.3719404, 0.1298125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2250] Time for 2250 iterations: 18.2 mins, Train Loss: (0.07098109, 2.321346, 0.1123125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2375] Time for 2375 iterations: 19.1 mins, Train Loss: (0.0271759, 2.2140558, 0.131125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2500] Time for 2500 iterations: 20.1 mins, Train Loss: (0.021325829, 2.1194215, 0.167875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2625] Time for 2625 iterations: 21.1 mins, Train Loss: (0.022261374, 2.0877953, 0.1775)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2750] Time for 2750 iterations: 22.1 mins, Train Loss: (0.019113012, 2.060927, 0.1896875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[2875] Time for 2875 iterations: 23.1 mins, Train Loss: (0.030653672, 2.082727, 0.199875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3000] Time for 3000 iterations: 24.2 mins, Train Loss: (0.040653583, 2.1259534, 0.172875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3125] Time for 3125 iterations: 25.2 mins, Train Loss: (0.025686333, 2.0535038, 0.2116875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3250] Time for 3250 iterations: 26.2 mins, Train Loss: (0.024101041, 2.0477073, 0.214)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3375] Time for 3375 iterations: 27.2 mins, Train Loss: (0.021512661, 1.9696991, 0.246625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3500] Time for 3500 iterations: 28.2 mins, Train Loss: (0.021673445, 1.9452684, 0.2565625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3625] Time for 3625 iterations: 29.2 mins, Train Loss: (0.051269952, 1.923274, 0.286625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3750] Time for 3750 iterations: 30.1 mins, Train Loss: (0.034370147, 1.9341115, 0.282)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3875] Time for 3875 iterations: 31.2 mins, Train Loss: (0.01762039, 1.7862012, 0.32)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4000] Time for 4000 iterations: 32.2 mins, Train Loss: (0.06944601, 1.8670467, 0.321375)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4125] Time for 4125 iterations: 33.2 mins, Train Loss: (0.05806866, 1.9624382, 0.2508125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4250] Time for 4250 iterations: 34.2 mins, Train Loss: (0.04221194, 1.8110776, 0.298625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4375] Time for 4375 iterations: 35.2 mins, Train Loss: (0.04311596, 1.7745041, 0.3171875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4500] Time for 4500 iterations: 36.2 mins, Train Loss: (0.04808716, 1.79188, 0.3151875)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4625] Time for 4625 iterations: 37.2 mins, Train Loss: (0.018905096, 1.7907647, 0.322)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4750] Time for 4750 iterations: 38.2 mins, Train Loss: (0.018301405, 1.6965859, 0.357625)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4875] Time for 4875 iterations: 39.3 mins, Train Loss: (0.014638876, 1.5744876, 0.4123125)\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5000] Time for 5000 iterations: 40.3 mins, Train Loss: (0.012508988, 1.5270255, 0.4378125)\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 125 # interval for evaluating on one-shot tasks\n",
    "n_iter = 16_000 # No. of training iterations\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "n_iteration=0\n",
    "\n",
    "loss_list = []\n",
    "total_loss = []\n",
    "\n",
    "emb_vals = []\n",
    "cat_vals = []\n",
    "acc_vals = []\n",
    "\n",
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "t_start = time.time()\n",
    "\n",
    "for i in range(1, n_iter+1):\n",
    "    \n",
    "    batch = train_generator.__next__()\n",
    "    \n",
    "    loss = model.train_on_batch(batch[0], batch[1])\n",
    "    \n",
    "    emb_vals.append(loss[1])\n",
    "    cat_vals.append(loss[2])\n",
    "    acc_vals.append(loss[4])\n",
    "    \n",
    "    n_iteration += 1\n",
    "    \n",
    "    if i % 4000 == 0:\n",
    "        K.set_value(model.optimizer.lr, K.get_value(model.optimizer.lr)/2.0)\n",
    "    \n",
    "    if i % evaluate_every == 0:\n",
    "        ploss = (np.mean(emb_vals), np.mean(cat_vals), np.mean(acc_vals))\n",
    "        loss_list.append(ploss)\n",
    "        emb_vals = []\n",
    "        cat_vals = []\n",
    "        acc_vals = []\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"[{3}] Time for {0} iterations: {1:.1f} mins, Train Loss: {2}\".format(i, (time.time()-t_start)/60.0,ploss,n_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.4125548377633095\n",
      "emb_out_loss 0.015087807085365057\n",
      "cat_out_loss 1.7668695077300072\n",
      "emb_out_acc 1.0\n",
      "cat_out_acc 0.4765625\n"
     ]
    }
   ],
   "source": [
    "batch = train_generator.__next__()\n",
    "\n",
    "loss = model.evaluate(batch[0], batch[1], verbose=0)\n",
    "\n",
    "for i, l in enumerate(model.metrics_names):\n",
    "    print(l, loss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base images:\n",
      "loss 0.5653817445755005\n",
      "acc 0.8494\n",
      "\n",
      "Adv images:\n",
      "loss 2.0614436777830125\n",
      "acc 0.2865\n"
     ]
    }
   ],
   "source": [
    "eval_loss = adv_model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Base images:\")\n",
    "for i, name in enumerate(adv_model.metrics_names):\n",
    "    print(name, eval_loss[i])\n",
    "\n",
    "losses = []\n",
    "accs = []\n",
    "\n",
    "\n",
    "for i in range(0, 8000, 32):\n",
    "    adv_test = attack.generate(x_test[i:i+32])\n",
    "    eval_loss = adv_model.evaluate(adv_test, y_test[i:i+32], verbose=0)\n",
    "    losses.append(eval_loss[0])\n",
    "    accs.append(eval_loss[1])\n",
    "\n",
    "print(\"\\nAdv images:\")\n",
    "print('loss', np.mean(losses))\n",
    "print('acc', np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
